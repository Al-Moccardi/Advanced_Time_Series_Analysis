{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Np6RCC8nbAJ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.losses import MeanSquaredLogarithmicError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL7Kir1Jvwzc"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "ATDPsAdxnsr6",
        "outputId": "c5796a79-62ad-4fa8-d86b-6854cf6265d1"
      },
      "source": [
        "# Download the dataset\n",
        "PATH_TO_DATA = 'http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv'\n",
        "data = pd.read_csv(PATH_TO_DATA, header=None)\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4998, 141)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.112522</td>\n",
              "      <td>-2.827204</td>\n",
              "      <td>-3.773897</td>\n",
              "      <td>-4.349751</td>\n",
              "      <td>-4.376041</td>\n",
              "      <td>-3.474986</td>\n",
              "      <td>-2.181408</td>\n",
              "      <td>-1.818287</td>\n",
              "      <td>-1.250522</td>\n",
              "      <td>-0.477492</td>\n",
              "      <td>-0.363808</td>\n",
              "      <td>-0.491957</td>\n",
              "      <td>-0.421855</td>\n",
              "      <td>-0.309201</td>\n",
              "      <td>-0.495939</td>\n",
              "      <td>-0.342119</td>\n",
              "      <td>-0.355336</td>\n",
              "      <td>-0.367913</td>\n",
              "      <td>-0.316503</td>\n",
              "      <td>-0.412374</td>\n",
              "      <td>-0.471672</td>\n",
              "      <td>-0.413458</td>\n",
              "      <td>-0.364617</td>\n",
              "      <td>-0.449298</td>\n",
              "      <td>-0.471419</td>\n",
              "      <td>-0.424777</td>\n",
              "      <td>-0.462517</td>\n",
              "      <td>-0.552472</td>\n",
              "      <td>-0.475375</td>\n",
              "      <td>-0.694200</td>\n",
              "      <td>-0.701868</td>\n",
              "      <td>-0.593812</td>\n",
              "      <td>-0.660684</td>\n",
              "      <td>-0.713831</td>\n",
              "      <td>-0.769807</td>\n",
              "      <td>-0.672282</td>\n",
              "      <td>-0.653676</td>\n",
              "      <td>-0.639406</td>\n",
              "      <td>-0.559302</td>\n",
              "      <td>-0.591670</td>\n",
              "      <td>...</td>\n",
              "      <td>1.258179</td>\n",
              "      <td>1.433789</td>\n",
              "      <td>1.700533</td>\n",
              "      <td>1.999043</td>\n",
              "      <td>2.125341</td>\n",
              "      <td>1.993291</td>\n",
              "      <td>1.932246</td>\n",
              "      <td>1.797437</td>\n",
              "      <td>1.522284</td>\n",
              "      <td>1.251168</td>\n",
              "      <td>0.998730</td>\n",
              "      <td>0.483722</td>\n",
              "      <td>0.023132</td>\n",
              "      <td>-0.194914</td>\n",
              "      <td>-0.220917</td>\n",
              "      <td>-0.243737</td>\n",
              "      <td>-0.254695</td>\n",
              "      <td>-0.291136</td>\n",
              "      <td>-0.256490</td>\n",
              "      <td>-0.227874</td>\n",
              "      <td>-0.322423</td>\n",
              "      <td>-0.289286</td>\n",
              "      <td>-0.318170</td>\n",
              "      <td>-0.363654</td>\n",
              "      <td>-0.393456</td>\n",
              "      <td>-0.266419</td>\n",
              "      <td>-0.256823</td>\n",
              "      <td>-0.288694</td>\n",
              "      <td>-0.162338</td>\n",
              "      <td>0.160348</td>\n",
              "      <td>0.792168</td>\n",
              "      <td>0.933541</td>\n",
              "      <td>0.796958</td>\n",
              "      <td>0.578621</td>\n",
              "      <td>0.257740</td>\n",
              "      <td>0.228077</td>\n",
              "      <td>0.123431</td>\n",
              "      <td>0.925286</td>\n",
              "      <td>0.193137</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.100878</td>\n",
              "      <td>-3.996840</td>\n",
              "      <td>-4.285843</td>\n",
              "      <td>-4.506579</td>\n",
              "      <td>-4.022377</td>\n",
              "      <td>-3.234368</td>\n",
              "      <td>-1.566126</td>\n",
              "      <td>-0.992258</td>\n",
              "      <td>-0.754680</td>\n",
              "      <td>0.042321</td>\n",
              "      <td>0.148951</td>\n",
              "      <td>0.183527</td>\n",
              "      <td>0.294876</td>\n",
              "      <td>0.190233</td>\n",
              "      <td>0.235575</td>\n",
              "      <td>0.253487</td>\n",
              "      <td>0.221742</td>\n",
              "      <td>0.050233</td>\n",
              "      <td>0.178042</td>\n",
              "      <td>0.139563</td>\n",
              "      <td>0.046794</td>\n",
              "      <td>0.043007</td>\n",
              "      <td>0.106544</td>\n",
              "      <td>0.012654</td>\n",
              "      <td>0.003995</td>\n",
              "      <td>0.045724</td>\n",
              "      <td>-0.045999</td>\n",
              "      <td>-0.072667</td>\n",
              "      <td>-0.071078</td>\n",
              "      <td>-0.153866</td>\n",
              "      <td>-0.227254</td>\n",
              "      <td>-0.249270</td>\n",
              "      <td>-0.253489</td>\n",
              "      <td>-0.332835</td>\n",
              "      <td>-0.264330</td>\n",
              "      <td>-0.345825</td>\n",
              "      <td>-0.310781</td>\n",
              "      <td>-0.334160</td>\n",
              "      <td>-0.306178</td>\n",
              "      <td>-0.174563</td>\n",
              "      <td>...</td>\n",
              "      <td>1.808428</td>\n",
              "      <td>2.164346</td>\n",
              "      <td>2.070747</td>\n",
              "      <td>1.903614</td>\n",
              "      <td>1.764455</td>\n",
              "      <td>1.507769</td>\n",
              "      <td>1.293428</td>\n",
              "      <td>0.894562</td>\n",
              "      <td>0.578016</td>\n",
              "      <td>0.244343</td>\n",
              "      <td>-0.286443</td>\n",
              "      <td>-0.515881</td>\n",
              "      <td>-0.732707</td>\n",
              "      <td>-0.832465</td>\n",
              "      <td>-0.803318</td>\n",
              "      <td>-0.836252</td>\n",
              "      <td>-0.777865</td>\n",
              "      <td>-0.774753</td>\n",
              "      <td>-0.733404</td>\n",
              "      <td>-0.721386</td>\n",
              "      <td>-0.832095</td>\n",
              "      <td>-0.711982</td>\n",
              "      <td>-0.751867</td>\n",
              "      <td>-0.757720</td>\n",
              "      <td>-0.853120</td>\n",
              "      <td>-0.766988</td>\n",
              "      <td>-0.688161</td>\n",
              "      <td>-0.519923</td>\n",
              "      <td>0.039406</td>\n",
              "      <td>0.560327</td>\n",
              "      <td>0.538356</td>\n",
              "      <td>0.656881</td>\n",
              "      <td>0.787490</td>\n",
              "      <td>0.724046</td>\n",
              "      <td>0.555784</td>\n",
              "      <td>0.476333</td>\n",
              "      <td>0.773820</td>\n",
              "      <td>1.119621</td>\n",
              "      <td>-1.436250</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.567088</td>\n",
              "      <td>-2.593450</td>\n",
              "      <td>-3.874230</td>\n",
              "      <td>-4.584095</td>\n",
              "      <td>-4.187449</td>\n",
              "      <td>-3.151462</td>\n",
              "      <td>-1.742940</td>\n",
              "      <td>-1.490658</td>\n",
              "      <td>-1.183580</td>\n",
              "      <td>-0.394229</td>\n",
              "      <td>-0.282897</td>\n",
              "      <td>-0.356926</td>\n",
              "      <td>-0.287297</td>\n",
              "      <td>-0.399489</td>\n",
              "      <td>-0.473244</td>\n",
              "      <td>-0.379048</td>\n",
              "      <td>-0.399039</td>\n",
              "      <td>-0.178594</td>\n",
              "      <td>-0.339522</td>\n",
              "      <td>-0.498447</td>\n",
              "      <td>-0.337251</td>\n",
              "      <td>-0.425480</td>\n",
              "      <td>-0.423952</td>\n",
              "      <td>-0.463170</td>\n",
              "      <td>-0.493253</td>\n",
              "      <td>-0.549749</td>\n",
              "      <td>-0.529831</td>\n",
              "      <td>-0.530935</td>\n",
              "      <td>-0.502365</td>\n",
              "      <td>-0.417368</td>\n",
              "      <td>-0.526346</td>\n",
              "      <td>-0.471005</td>\n",
              "      <td>-0.676784</td>\n",
              "      <td>-0.898612</td>\n",
              "      <td>-0.610571</td>\n",
              "      <td>-0.530164</td>\n",
              "      <td>-0.765674</td>\n",
              "      <td>-0.581937</td>\n",
              "      <td>-0.537848</td>\n",
              "      <td>-0.556386</td>\n",
              "      <td>...</td>\n",
              "      <td>1.810988</td>\n",
              "      <td>2.185398</td>\n",
              "      <td>2.262985</td>\n",
              "      <td>2.052920</td>\n",
              "      <td>1.890488</td>\n",
              "      <td>1.793033</td>\n",
              "      <td>1.564784</td>\n",
              "      <td>1.234619</td>\n",
              "      <td>0.900302</td>\n",
              "      <td>0.551957</td>\n",
              "      <td>0.258222</td>\n",
              "      <td>-0.128587</td>\n",
              "      <td>-0.092585</td>\n",
              "      <td>-0.168606</td>\n",
              "      <td>-0.495989</td>\n",
              "      <td>-0.395034</td>\n",
              "      <td>-0.328238</td>\n",
              "      <td>-0.448138</td>\n",
              "      <td>-0.268230</td>\n",
              "      <td>-0.456415</td>\n",
              "      <td>-0.357867</td>\n",
              "      <td>-0.317508</td>\n",
              "      <td>-0.434112</td>\n",
              "      <td>-0.549203</td>\n",
              "      <td>-0.324615</td>\n",
              "      <td>-0.268082</td>\n",
              "      <td>-0.220384</td>\n",
              "      <td>-0.117429</td>\n",
              "      <td>0.614059</td>\n",
              "      <td>1.284825</td>\n",
              "      <td>0.886073</td>\n",
              "      <td>0.531452</td>\n",
              "      <td>0.311377</td>\n",
              "      <td>-0.021919</td>\n",
              "      <td>-0.713683</td>\n",
              "      <td>-0.532197</td>\n",
              "      <td>0.321097</td>\n",
              "      <td>0.904227</td>\n",
              "      <td>-0.421797</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.490473</td>\n",
              "      <td>-1.914407</td>\n",
              "      <td>-3.616364</td>\n",
              "      <td>-4.318823</td>\n",
              "      <td>-4.268016</td>\n",
              "      <td>-3.881110</td>\n",
              "      <td>-2.993280</td>\n",
              "      <td>-1.671131</td>\n",
              "      <td>-1.333884</td>\n",
              "      <td>-0.965629</td>\n",
              "      <td>-0.183319</td>\n",
              "      <td>-0.101657</td>\n",
              "      <td>-0.273874</td>\n",
              "      <td>-0.127818</td>\n",
              "      <td>-0.195983</td>\n",
              "      <td>-0.213523</td>\n",
              "      <td>-0.176473</td>\n",
              "      <td>-0.156932</td>\n",
              "      <td>-0.149172</td>\n",
              "      <td>-0.181510</td>\n",
              "      <td>-0.180074</td>\n",
              "      <td>-0.246151</td>\n",
              "      <td>-0.274260</td>\n",
              "      <td>-0.140960</td>\n",
              "      <td>-0.277449</td>\n",
              "      <td>-0.382549</td>\n",
              "      <td>-0.311937</td>\n",
              "      <td>-0.360093</td>\n",
              "      <td>-0.405968</td>\n",
              "      <td>-0.571433</td>\n",
              "      <td>-0.524106</td>\n",
              "      <td>-0.537886</td>\n",
              "      <td>-0.606778</td>\n",
              "      <td>-0.661446</td>\n",
              "      <td>-0.683375</td>\n",
              "      <td>-0.746683</td>\n",
              "      <td>-0.635662</td>\n",
              "      <td>-0.625231</td>\n",
              "      <td>-0.540094</td>\n",
              "      <td>-0.674995</td>\n",
              "      <td>...</td>\n",
              "      <td>1.772155</td>\n",
              "      <td>2.000769</td>\n",
              "      <td>1.925003</td>\n",
              "      <td>1.898426</td>\n",
              "      <td>1.720953</td>\n",
              "      <td>1.501711</td>\n",
              "      <td>1.422492</td>\n",
              "      <td>1.023225</td>\n",
              "      <td>0.776341</td>\n",
              "      <td>0.504426</td>\n",
              "      <td>0.056382</td>\n",
              "      <td>-0.233161</td>\n",
              "      <td>-0.406388</td>\n",
              "      <td>-0.327528</td>\n",
              "      <td>-0.460868</td>\n",
              "      <td>-0.402536</td>\n",
              "      <td>-0.345752</td>\n",
              "      <td>-0.354206</td>\n",
              "      <td>-0.439959</td>\n",
              "      <td>-0.425326</td>\n",
              "      <td>-0.439789</td>\n",
              "      <td>-0.451835</td>\n",
              "      <td>-0.395926</td>\n",
              "      <td>-0.448762</td>\n",
              "      <td>-0.391789</td>\n",
              "      <td>-0.376307</td>\n",
              "      <td>-0.461069</td>\n",
              "      <td>-0.253524</td>\n",
              "      <td>0.213006</td>\n",
              "      <td>0.491173</td>\n",
              "      <td>0.350816</td>\n",
              "      <td>0.499111</td>\n",
              "      <td>0.600345</td>\n",
              "      <td>0.842069</td>\n",
              "      <td>0.952074</td>\n",
              "      <td>0.990133</td>\n",
              "      <td>1.086798</td>\n",
              "      <td>1.403011</td>\n",
              "      <td>-0.383564</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.800232</td>\n",
              "      <td>-0.874252</td>\n",
              "      <td>-2.384761</td>\n",
              "      <td>-3.973292</td>\n",
              "      <td>-4.338224</td>\n",
              "      <td>-3.802422</td>\n",
              "      <td>-2.534510</td>\n",
              "      <td>-1.783423</td>\n",
              "      <td>-1.594450</td>\n",
              "      <td>-0.753199</td>\n",
              "      <td>-0.298107</td>\n",
              "      <td>-0.428928</td>\n",
              "      <td>-0.491351</td>\n",
              "      <td>-0.361304</td>\n",
              "      <td>-0.339296</td>\n",
              "      <td>-0.324952</td>\n",
              "      <td>-0.290113</td>\n",
              "      <td>-0.363051</td>\n",
              "      <td>-0.525684</td>\n",
              "      <td>-0.597423</td>\n",
              "      <td>-0.575523</td>\n",
              "      <td>-0.567503</td>\n",
              "      <td>-0.504555</td>\n",
              "      <td>-0.618406</td>\n",
              "      <td>-0.682814</td>\n",
              "      <td>-0.743849</td>\n",
              "      <td>-0.815588</td>\n",
              "      <td>-0.826902</td>\n",
              "      <td>-0.782374</td>\n",
              "      <td>-0.929462</td>\n",
              "      <td>-0.999672</td>\n",
              "      <td>-1.060969</td>\n",
              "      <td>-1.007877</td>\n",
              "      <td>-1.028735</td>\n",
              "      <td>-1.122629</td>\n",
              "      <td>-1.028650</td>\n",
              "      <td>-1.046515</td>\n",
              "      <td>-1.063372</td>\n",
              "      <td>-1.122423</td>\n",
              "      <td>-0.983242</td>\n",
              "      <td>...</td>\n",
              "      <td>1.155363</td>\n",
              "      <td>1.336254</td>\n",
              "      <td>1.627534</td>\n",
              "      <td>1.717594</td>\n",
              "      <td>1.696487</td>\n",
              "      <td>1.741686</td>\n",
              "      <td>1.674078</td>\n",
              "      <td>1.546928</td>\n",
              "      <td>1.331738</td>\n",
              "      <td>1.110168</td>\n",
              "      <td>0.922210</td>\n",
              "      <td>0.521777</td>\n",
              "      <td>0.154852</td>\n",
              "      <td>-0.123861</td>\n",
              "      <td>-0.202998</td>\n",
              "      <td>-0.247956</td>\n",
              "      <td>-0.219122</td>\n",
              "      <td>-0.214695</td>\n",
              "      <td>-0.319215</td>\n",
              "      <td>-0.198597</td>\n",
              "      <td>-0.151618</td>\n",
              "      <td>-0.129593</td>\n",
              "      <td>-0.074939</td>\n",
              "      <td>-0.196807</td>\n",
              "      <td>-0.174795</td>\n",
              "      <td>-0.208833</td>\n",
              "      <td>-0.210754</td>\n",
              "      <td>-0.100485</td>\n",
              "      <td>0.197446</td>\n",
              "      <td>0.966606</td>\n",
              "      <td>1.148884</td>\n",
              "      <td>0.958434</td>\n",
              "      <td>1.059025</td>\n",
              "      <td>1.371682</td>\n",
              "      <td>1.277392</td>\n",
              "      <td>0.960304</td>\n",
              "      <td>0.971020</td>\n",
              "      <td>1.614392</td>\n",
              "      <td>1.421456</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 141 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2         3    ...       137       138       139  140\n",
              "0 -0.112522 -2.827204 -3.773897 -4.349751  ...  0.123431  0.925286  0.193137  1.0\n",
              "1 -1.100878 -3.996840 -4.285843 -4.506579  ...  0.773820  1.119621 -1.436250  1.0\n",
              "2 -0.567088 -2.593450 -3.874230 -4.584095  ...  0.321097  0.904227 -0.421797  1.0\n",
              "3  0.490473 -1.914407 -3.616364 -4.318823  ...  1.086798  1.403011 -0.383564  1.0\n",
              "4  0.800232 -0.874252 -2.384761 -3.973292  ...  0.971020  1.614392  1.421456  1.0\n",
              "\n",
              "[5 rows x 141 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV6KUkRgvoOK"
      },
      "source": [
        "## Split the data for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTEDLRtA4I9k"
      },
      "source": [
        "# last column is the target\n",
        "# 0 = anomaly, 1 = normal\n",
        "TARGET = 140\n",
        "\n",
        "features = data.drop(TARGET, axis=1)\n",
        "target = data[TARGET]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    features, target, test_size=0.2, stratify=target\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mjzHNYjLviR"
      },
      "source": [
        "# use case is novelty detection so use only the normal data\n",
        "# for training\n",
        "train_index = y_train[y_train == 1].index\n",
        "train_data = x_train.loc[train_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfk5mwbhv03v"
      },
      "source": [
        "## Scale the data using MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmJhTiuBts4F"
      },
      "source": [
        "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "x_train_scaled = min_max_scaler.fit_transform(train_data.copy())\n",
        "x_test_scaled = min_max_scaler.transform(x_test.copy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFLk9eJrwqvl"
      },
      "source": [
        "## Build an AutoEncoder model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9VmLu89uPIY"
      },
      "source": [
        "# create a model by subclassing Model class in tensorflow\n",
        "class AutoEncoder(Model):\n",
        "  \"\"\"\n",
        "  Parameters\n",
        "  ----------\n",
        "  output_units: int\n",
        "    Number of output units\n",
        "\n",
        "  code_size: int\n",
        "    Number of units in bottle neck\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, output_units, code_size=8):\n",
        "    super().__init__()\n",
        "    self.encoder = Sequential([\n",
        "      Dense(64, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(32, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(16, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(code_size, activation='relu')\n",
        "    ])\n",
        "    self.decoder = Sequential([\n",
        "      Dense(16, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(32, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(64, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(output_units, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    encoded = self.encoder(inputs)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oylLrsQuPD5",
        "outputId": "9e8b92a5-2507-4f5e-a9a1-33c398d36a70"
      },
      "source": [
        "model = AutoEncoder(output_units=x_train_scaled.shape[1])\n",
        "# configurations of model\n",
        "model.compile(loss='msle', metrics=['mse'], optimizer='adam')\n",
        "\n",
        "history = model.fit(\n",
        "    x_train_scaled,\n",
        "    x_train_scaled,\n",
        "    epochs=20,\n",
        "    batch_size=512,\n",
        "    validation_data=(x_test_scaled, x_test_scaled)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0110 - mse: 0.0244 - val_loss: 0.0136 - val_mse: 0.0320\n",
            "Epoch 2/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0106 - mse: 0.0235 - val_loss: 0.0134 - val_mse: 0.0314\n",
            "Epoch 3/20\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0102 - mse: 0.0226 - val_loss: 0.0131 - val_mse: 0.0306\n",
            "Epoch 4/20\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0095 - mse: 0.0211 - val_loss: 0.0128 - val_mse: 0.0300\n",
            "Epoch 5/20\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0088 - mse: 0.0194 - val_loss: 0.0125 - val_mse: 0.0293\n",
            "Epoch 6/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0080 - mse: 0.0177 - val_loss: 0.0122 - val_mse: 0.0286\n",
            "Epoch 7/20\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0072 - mse: 0.0159 - val_loss: 0.0119 - val_mse: 0.0279\n",
            "Epoch 8/20\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0065 - mse: 0.0144 - val_loss: 0.0114 - val_mse: 0.0270\n",
            "Epoch 9/20\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0060 - mse: 0.0132 - val_loss: 0.0110 - val_mse: 0.0261\n",
            "Epoch 10/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0056 - mse: 0.0123 - val_loss: 0.0107 - val_mse: 0.0254\n",
            "Epoch 11/20\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0053 - mse: 0.0117 - val_loss: 0.0106 - val_mse: 0.0251\n",
            "Epoch 12/20\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0050 - mse: 0.0110 - val_loss: 0.0105 - val_mse: 0.0250\n",
            "Epoch 13/20\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0049 - mse: 0.0108 - val_loss: 0.0105 - val_mse: 0.0249\n",
            "Epoch 14/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0048 - mse: 0.0105 - val_loss: 0.0105 - val_mse: 0.0249\n",
            "Epoch 15/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0049 - mse: 0.0107 - val_loss: 0.0105 - val_mse: 0.0249\n",
            "Epoch 16/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0048 - mse: 0.0105 - val_loss: 0.0105 - val_mse: 0.0249\n",
            "Epoch 17/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0048 - mse: 0.0106 - val_loss: 0.0104 - val_mse: 0.0248\n",
            "Epoch 18/20\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0047 - mse: 0.0104 - val_loss: 0.0104 - val_mse: 0.0246\n",
            "Epoch 19/20\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0045 - mse: 0.0099 - val_loss: 0.0104 - val_mse: 0.0246\n",
            "Epoch 20/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0045 - mse: 0.0099 - val_loss: 0.0103 - val_mse: 0.0245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCD6ykuG6wvV"
      },
      "source": [
        "## Plot history"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "ohEpArnj6NMG",
        "outputId": "9e513350-cbc3-4602-ddc6-7e8104302d2a"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSLE Loss')\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPNtOnsENWwc"
      },
      "source": [
        "## Find threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACFZcFWEFNpQ"
      },
      "source": [
        "def find_threshold(model, x_train_scaled):\n",
        "  reconstructions = model.predict(x_train_scaled)\n",
        "  # provides losses of individual instances\n",
        "  reconstruction_errors = tf.keras.losses.msle(reconstructions, x_train_scaled)\n",
        "\n",
        "  # threshold for anomaly scores\n",
        "  threshold = np.mean(reconstruction_errors.numpy()) \\\n",
        "      + np.std(reconstruction_errors.numpy())\n",
        "  return threshold\n",
        "\n",
        "def find_threshold_method_two(model, x_train_scaled):\n",
        "  # another method to find threshold\n",
        "  reconstructions = model.predict(x_train_scaled)\n",
        "  # provides losses of individual instances\n",
        "  reconstruction_errors = tf.keras.losses.msle(reconstructions, x_train_scaled)\n",
        "\n",
        "  threshold_2 = np.percentile(reconstruction_errors, 95)\n",
        "  return threshold_2\n",
        "\n",
        "def get_predictions(model, x_test_scaled, threshold):\n",
        "  predictions = model.predict(x_test_scaled)\n",
        "  # provides losses of individual instances\n",
        "  errors = tf.keras.losses.msle(predictions, x_test_scaled)\n",
        "  # 0 = anomaly, 1 = normal\n",
        "  anomaly_mask = pd.Series(errors) > threshold\n",
        "  preds = anomaly_mask.map(lambda x: 0.0 if x == True else 1.0)\n",
        "  return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4Mxr7cnHPk-",
        "outputId": "44e0954e-fa48-47ed-e0bb-9276cb67aa5d"
      },
      "source": [
        "threshold = find_threshold(model, x_train_scaled)\n",
        "print(f\"Threshold method one: {threshold}\")\n",
        "\n",
        "threshold_2 = find_threshold_method_two(model, x_train_scaled)\n",
        "print(f\"Threshold method two: {threshold_2}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Threshold method one: 0.01001314025746261\n",
            "Threshold method two: 0.014635060198344029\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl0Op0GNHVDD",
        "outputId": "81efd337-cd8e-47e0-e4ca-d0e19c1ef2e1"
      },
      "source": [
        "preds = get_predictions(model, x_test_scaled, threshold)\n",
        "accuracy_score(preds, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.944"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TSdu3Uk7ASm"
      },
      "source": [
        "## Tuning AutoEncoder using keras tuner\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Zt5CaPNCh__",
        "outputId": "66ce8062-88c7-4967-a538-1736f70cf5ca"
      },
      "source": [
        "!pip install -U keras-tuner"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: keras-tuner in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: terminaltables in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.8.9)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: colorama in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AwQid85DbyC"
      },
      "source": [
        "import kerastuner as kt\n",
        "\n",
        "class AutoEncoderTuner(Model):\n",
        "\n",
        "  def __init__(self, hp, output_units, code_size=8):\n",
        "    super().__init__()\n",
        "    dense_1_units = hp.Int('dense_1_units', min_value=16, max_value=72, step=4)\n",
        "    dense_2_units = hp.Int('dense_2_units', min_value=16, max_value=72, step=4)\n",
        "    dense_3_units = hp.Int('dense_3_units', min_value=16, max_value=72, step=4)\n",
        "    dense_4_units = hp.Int('dense_4_units', min_value=16, max_value=72, step=4)\n",
        "    dense_5_units = hp.Int('dense_5_units', min_value=16, max_value=72, step=4)\n",
        "    dense_6_units = hp.Int('dense_6_units', min_value=16, max_value=72, step=4)\n",
        "\n",
        "    self.encoder = Sequential([\n",
        "      Dense(dense_1_units, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(dense_2_units, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(dense_3_units, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(code_size, activation='relu')\n",
        "    ])\n",
        "    self.decoder = Sequential([\n",
        "      Dense(dense_4_units, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(dense_5_units, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(dense_6_units, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(output_units, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    encoded = self.encoder(inputs)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "\n",
        "def build_model(hp):\n",
        "  model = AutoEncoderTuner(hp, 140)\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  model.compile(\n",
        "      loss='msle',\n",
        "      optimizer=Adam(learning_rate=hp_learning_rate),\n",
        "  )\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6X0KEcX6NAS",
        "outputId": "8bf5e44c-ed99-40b3-a051-68aad31fce9e"
      },
      "source": [
        "tuner = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_epochs=20,\n",
        "    factor=3,\n",
        "    directory='autoencoder',\n",
        "    project_name='tuning_autoencoder6'\n",
        ")\n",
        "\n",
        "tuner.search(\n",
        "    x_train_scaled,\n",
        "    x_train_scaled,\n",
        "    epochs=20,\n",
        "    batch_size=512,\n",
        "    validation_data=(x_test_scaled, x_test_scaled)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Oracle from existing project autoencoder/tuning_autoencoder6/oracle.json\n",
            "INFO:tensorflow:Reloading Tuner from autoencoder/tuning_autoencoder6/tuner0.json\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tiB8zhcF0QV",
        "outputId": "4e5b74d2-fff8-4502-f60b-d15f388330ce"
      },
      "source": [
        "hparams = [f'dense_{i}_units' for i in range(1,7)] + ['learning_rate']\n",
        "best_hyperparams = tuner.get_best_hyperparameters()\n",
        "for hps in hparams:\n",
        "  print(f\"{hps}: {best_hyperparams[0][hps]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dense_1_units: 40\n",
            "dense_2_units: 32\n",
            "dense_3_units: 16\n",
            "dense_4_units: 44\n",
            "dense_5_units: 36\n",
            "dense_6_units: 36\n",
            "learning_rate: 0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUvDIqvVEuNk",
        "outputId": "0c8f1a84-c746-4f3d-fd12-a3dade4e5459"
      },
      "source": [
        "best_model = tuner.get_best_models()[0]\n",
        "best_model.compile(loss='msle', optimizer=Adam(0.001))\n",
        "\n",
        "best_model.fit(\n",
        "    x_train_scaled,\n",
        "    x_train_scaled,\n",
        "    epochs=20,\n",
        "    batch_size=512,\n",
        "    validation_data=(x_test_scaled, x_test_scaled)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0058 - val_loss: 0.0108\n",
            "Epoch 2/20\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0052 - val_loss: 0.0105\n",
            "Epoch 3/20\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0048 - val_loss: 0.0103\n",
            "Epoch 4/20\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0046 - val_loss: 0.0101\n",
            "Epoch 5/20\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0044 - val_loss: 0.0099\n",
            "Epoch 6/20\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0043 - val_loss: 0.0098\n",
            "Epoch 7/20\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0041 - val_loss: 0.0098\n",
            "Epoch 8/20\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0040 - val_loss: 0.0097\n",
            "Epoch 9/20\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0040 - val_loss: 0.0097\n",
            "Epoch 10/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0038 - val_loss: 0.0096\n",
            "Epoch 11/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0038 - val_loss: 0.0096\n",
            "Epoch 12/20\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0037 - val_loss: 0.0095\n",
            "Epoch 13/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0036 - val_loss: 0.0095\n",
            "Epoch 14/20\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0035 - val_loss: 0.0095\n",
            "Epoch 15/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0035 - val_loss: 0.0094\n",
            "Epoch 16/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0034 - val_loss: 0.0094\n",
            "Epoch 17/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0034 - val_loss: 0.0094\n",
            "Epoch 18/20\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0033 - val_loss: 0.0094\n",
            "Epoch 19/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0032 - val_loss: 0.0093\n",
            "Epoch 20/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0032 - val_loss: 0.0094\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f928c181090>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0DvGEOkHdsn",
        "outputId": "534ee8fa-d12d-49e4-b95e-0b5dbc24760d"
      },
      "source": [
        "threshold_ = find_threshold(best_model, x_train_scaled)\n",
        "preds_ = get_predictions(best_model, x_test_scaled, threshold_)\n",
        "accuracy_score(preds_, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.946"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}